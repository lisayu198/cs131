from pyspark.sql import SparkSession
from pyspark.sql.functions import avg
import time
import matplotlib.pyplot as plt

# Start Spark session
spark = SparkSession.builder \
    .appName("GenerateAndTestFiles") \
    .getOrCreate()

# --- Step 1: Generate larger files ---
print("Generating larger CSV files...")
base_path = "gs://dataproc-staging-us-central1-762478826489-su9ldjls/2019-01-h1.csv"
df = spark.read.csv(base_path, header=True, inferSchema=True)
multipliers = [1, 2, 4, 8, 16]

for m in multipliers:
    df_large = df
    for _ in range(m - 1):
        df_large = df_large.union(df)

    output_path = f"gs://dataproc-staging-us-central1-762478826489-su9ldjls/data_{m}x.csv"
    df_large.write.mode("overwrite").option("header", True).csv(output_path)
    print(f"Written: data_{m}x.csv")

# --- Step 2: Process files and measure runtime ---
print("\nRunning aggregation and recording runtimes...")
runtimes = []

for m in multipliers:
    file_path = f"gs://dataproc-staging-us-central1-762478826489-su9ldjls/data_{m}x.csv"
    print(f"\nProcessing data_{m}x.csv")

    start = time.time()
    
    df = spark.read.csv(file_path, header=True, inferSchema=True)
    df.groupBy("vendorid").avg("total_amount").show()
    
    end = time.time()
    elapsed = end - start
    runtimes.append(elapsed)
    print(f"Time taken: {elapsed:.2f} seconds")

spark.stop()

# --- Step 3: Plot the results ---
plt.plot(multipliers, runtimes, marker='o')
plt.title("Runtime vs File Size")
plt.xlabel("Data Size Multiplier (x)")
plt.ylabel("Runtime (seconds)")
plt.grid(True)
plt.show()

